# ============================================================================
# AI-Native MVP - Environment Variables Template
# ============================================================================
# Copy this file to .env and fill in your values
# NEVER commit .env to git (it's in .gitignore)
#
# REQUIRED variables (docker-compose will fail without these):
#   - POSTGRES_PASSWORD
#   - REDIS_PASSWORD
#   - JWT_SECRET_KEY
#   - SECRET_KEY
#
# Generate secrets with: python -c 'import secrets; print(secrets.token_urlsafe(32))'
# ============================================================================

# ============================================================================
# DATABASE (REQUIRED)
# ============================================================================
POSTGRES_DB=ai_native
POSTGRES_USER=ai_native
POSTGRES_PASSWORD=CHANGE_THIS_GENERATE_SECURE_PASSWORD

# Full connection URL (auto-constructed in docker-compose, but needed for local dev)
DATABASE_URL=postgresql://ai_native:${POSTGRES_PASSWORD}@localhost:5432/ai_native
DB_POOL_SIZE=80
DB_MAX_OVERFLOW=80
DB_POOL_TIMEOUT=5
DB_POOL_RECYCLE=3600

# ============================================================================
# REDIS CACHE (REQUIRED)
# ============================================================================
REDIS_PASSWORD=CHANGE_THIS_GENERATE_SECURE_PASSWORD
REDIS_URL=redis://:${REDIS_PASSWORD}@localhost:6379/0
LLM_CACHE_ENABLED=true
LLM_CACHE_TTL=3600
LLM_CACHE_MAX_ENTRIES=1000
# FIX 4.8: Add CACHE_SALT for cache key generation
CACHE_SALT=CHANGE_THIS_GENERATE_WITH_PYTHON_SECRETS

# ============================================================================
# LLM PROVIDER - OLLAMA (Local, Free, Privacy-First)
# ============================================================================
LLM_PROVIDER=ollama

# Ollama server URL (default: localhost)
# For Docker: http://ollama:11434
# For local: http://localhost:11434
OLLAMA_BASE_URL=http://localhost:11434

# Model to use (install with: ollama pull <model>)
# Options: phi3, llama2, mistral, codellama, gemma:7b, etc.
# Recommended: phi3 (Microsoft Phi-3: 3.8B params, fast, efficient)
OLLAMA_MODEL=phi3

# Generation temperature (0.0 = deterministic, 1.0 = creative)
OLLAMA_TEMPERATURE=0.7

# FIX 4.8: Add OLLAMA_TIMEOUT (was missing)
# Request timeout in seconds
OLLAMA_TIMEOUT=120

# ============================================================================
# ALTERNATIVE LLM PROVIDERS (Optional - choose ONE provider)
# ============================================================================
# If using OpenAI instead of Ollama:
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-your-openai-api-key-here

# If using Google Gemini:
# LLM_PROVIDER=gemini
# GEMINI_API_KEY=your-gemini-api-key-here

# If using Anthropic Claude:
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# ============================================================================
# SECURITY (REQUIRED)
# ============================================================================
# JWT Secret Key - REQUIRED, no default value
# Generate with: python -c 'import secrets; print(secrets.token_urlsafe(32))'
JWT_SECRET_KEY=CHANGE_THIS_GENERATE_WITH_PYTHON_SECRETS
SECRET_KEY=CHANGE_THIS_GENERATE_WITH_PYTHON_SECRETS

JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# ============================================================================
# APPLICATION
# ============================================================================
ENVIRONMENT=development
DEBUG=false
LOG_LEVEL=INFO

# ============================================================================
# CORS (Frontend origins)
# ============================================================================
# Include all frontend development ports (3000, 3001, 5173, 8080)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:3001,http://localhost:5173,http://localhost:8080

# ============================================================================
# MONITORING PROFILE (docker-compose --profile monitoring)
# FIX 1.2: Required when using monitoring profile
# ============================================================================
GRAFANA_USER=admin
GRAFANA_PASSWORD=CHANGE_THIS_GENERATE_SECURE_PASSWORD

# Prometheus doesn't require auth by default, but consider nginx proxy in production

# ============================================================================
# DEBUG PROFILE (docker-compose --profile debug)
# FIX 1.2, 1.3: Required when using debug profile
# ============================================================================
# pgAdmin credentials
PGADMIN_EMAIL=admin@ai-native.local
PGADMIN_PASSWORD=CHANGE_THIS_GENERATE_SECURE_PASSWORD

# Redis Commander credentials (FIX 1.3: HTTP basic auth)
REDIS_COMMANDER_USER=admin
REDIS_COMMANDER_PASSWORD=CHANGE_THIS_GENERATE_SECURE_PASSWORD

# ============================================================================
# QUICK START
# ============================================================================
# 1. Install Ollama: https://ollama.ai
# 2. Download a model: ollama pull llama2
# 3. Start Ollama: ollama serve
# 4. Copy this file: cp .env.example .env
# 5. Run Phoenix: python scripts/run_api.py
# 6. Access API docs: http://localhost:8000/docs
#
# For Docker deployment:
#   docker-compose --profile ollama up -d
#
# Documentation:
#   - Quick Start: OLLAMA_QUICKSTART.md
#   - Full Guide: OLLAMA_INTEGRATION_GUIDE.md
